<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Alan Dingtian Zhang</title>
<link href="main.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="tooltip.js"></script>
</head>

<body>
		<div id="title">
				<a><img src="images/photo.jpg" alt="" border="0" title="" /></a><br />
				<h1>"Alan" <font color="#045FB4"> Dingtian Zhang</font> &nbsp; <img src="images/handwriting.png" /> </h1>
                <!-- <p> -->
					<!-- dingtianzhang@gatech.edu -->
				<!-- </p> -->
                <!-- <a href="http://www.linkedin.com/in/alandzhang">LinkedIn</a> -->
                <a href="mailto:dzhang@hyperfine.io"><font color="#4198EF">[Email]</font></a>
                <a href="resources/Dingtian_Zhang_CV.pdf"><font color="#4198EF">[CV]</font></a>
                <a href="https://www.linkedin.com/in/alandzhang/">[LinkedIn]</a>
                <a href="https://twitter.com/AlanDZhang">[Twitter]</a>
		</div>
		<!-- <hr size="1" width="50%" color="#DDDDDD"> -->
		<!-- <div id="menu"> -->
				<!-- <a href="index.html"><font color="#000000">About</a> -->
				<!-- <a href="research.html">Research</a> -->
				<!-- <a href="fun.html">Interests</a> -->
		<!-- </div> -->
		<!-- <hr size="1" width="50%" color="#DDDDDD"> -->
		<!-- <br /> -->
		<div id="container">
				<!-- <div id="content"> -->
                <p>
				I am a Senior Scientist at Hyperfine working on human sensing and machine learning. I received my Computer Science Ph.D. at Ubicomp Group, Georgia Tech. My thesis was at the intersection of human-computer interaction, machine intelligence, and materials -- self-powered light sensing surfaces that enable implicit activity detection and explicit interactions on everyday surfaces. My research is fueled by novel materials and sensing techniques, intelligent user interfaces, and machine learning. </p>
                <p>I have received ACM IMWUT Distinguished Paper Award (2021) and <a href="https://crnch.gatech.edu/content/phd-fellowship-winners">CRNCH Ph.D. Research Fellowship (2020)</a>. I have also worked at Facebook, Disney Research, and Technicolor as research interns. <!-- <font face="Crimson Pro SemiBold"> Currently seeking full-time positions starting June 2021.</font> -->
                </p>
                
                <!-- </div> -->
        </div>
        
        <br />
		<hr size="1" width="50%" color="#DDDDDD">
		
		<!-- <div id="container"> -->

            <!-- <div id="sidebar2"> -->
				<!-- <h1><font color="#045FB4">Research</h1> -->
                <!-- <br /><br /> -->
            <!-- </div> -->
		<!-- </div> -->
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
        <div id="container">
			<div id="content2">
				<h1>Research Projects</h1>
			</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-npj.png" />						
							</div>
				<div id="content2">
						<h2>Flexible computational photodetectors for self-powered activity sensing (npj Flexible Electronics 2022)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							Computational photodetectors use in-sensor computation to extract mid-level vision features in the analog domain for low power and latency ubiquitous sensing applications. We adopt emerging organic semiconductor (OSC) devices in developing privacy-compliant large-scale sensing surfaces for implicit activity detection and explicit user interactions.
							<a href="https://doi.org/10.1038/s41528-022-00137-z" >[Paper]</a>
                            </p>
                            
							 <br />
                             
				</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-automobile.png" />						
							</div>
				<div id="content2">
						<h2>Applying Compute-Proximal Energy Harvesting to Develop Self-Sustained Systems for Automobiles (IEEE Pervasive Computing 2021)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							We apply computer-proximal energy (i.e., wind, light, vibration, and heat) harvesting in and around the automobile to provide power for intelligent sensors retrofitted to any automobile without wiring. We demonstrated a thermoelectric energybased parking assistant, which is attached to the exhaust pipe, and a wind-powered external pedestrian display, which is anchored to the front bumper of a car.
							<a href="https://doi.org/10.1109/MPRV.2021.3124738" >[Paper]</a>
                            </p>
                            
							 <br />
                             
				</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-optosense.png" />						
							</div>
				<div id="content2">
						<h2>OptoSense: Towards Ubiquitous Self-Powered Ambient Light Sensing Surfaces (Ubicomp 2020, <b>Distinguished Paper Award</b>)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							OptoSense is a general-purpose self-powered sensing system which senses ambient light at the surface level of everyday objects to infer user activities and interactions. We present a design framework of ambient light sensing surfaces, enabling implicit activity sensing and explicit interactions in a wide range of use cases with varying sensing dimensions (0D, 1D, 2D), fields of view (wide, narrow), and perspectives (egocentric, allocentric), which supports applications ranging from object use and indoor traffic detection, to liquid sensing and multitouch input.
							<a href="https://www.youtube.com/watch?v=A7FF3m8_b68" >[Video]</a>
							<a href="https://doi.org/10.1145/3411826" >[Paper]</a>
                            </p>
                            
							 <br />
                             
				</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-ubiquitouch.png" />						
							</div>
				<div id="content2">
						<h2>UbiquiTouch: Self Sustaining Ubiquitous Touch Interfaces (Ubicomp 2020)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							UbiquiTouch is an ultra low power wireless touch interface. With an average power consumption of less than 50 uW, UbiquiTouch can run on energy harvested from ambient light. It encodes touch events on a printable surface and passively communicates to a nearby smartphone using ambient FM backscatter. This approach minimizes the need for additional infrastructure for communication. 
							<a href="https://www.youtube.com/watch?v=W88H0mjAitA" >[Video]</a>
							<a href="https://doi.org/10.1145/3380989" >[Paper]</a>
                            </p>
                            
							 <br />
                             
				</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-serpentine.jpg" />					
							</div>
				<div id="content2">
						<h2>Serpentine: A Reversibly Deformable Cord Sensor for Human Input (CHI 2018)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							Serpentine is a self-powered reversibly deformable cord capable of sensing a variety of human input such as pluck, twirl, stretch, pinch, wiggle, and twist. The sensor operates without external power source based on the principle of Triboelectric Nanogenerators (TENG), and can be employed in wearable and playful interfaces.
							<a href="https://www.youtube.com/watch?v=oahuvZPG3tQ" >[Video]</a>
                            <a href="https://dl.acm.org/citation.cfm?id=3300775" >[Paper]</a>
						</p>
                        <br />
				</div>
		</div>
        
        <div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-blockprint.png" />						
							</div>
				<div id="content2">
						<h2>BlockPrint: Fabricating Interactive Board Books</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							BlockPrint is a fabrication pipeline that produces high-quality, colored board books with embedded interactivity. Each book page is fabricated by a commodity 3D paper-based printer with interactive color 3D structures, then embedded with a variety of sensing and actuatione elements for storytelling, and finally are bound into a book as a standalone system requiring no additional digital devices.
                            </p>
                            
							 <br />
				</div>
		</div>
        
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-whoosh.jpg" />				
							</div>
				<div id="content2">
						<h2>Whoosh: Non-Voice Acoustics for Low-Cost, Hands-Free, and Rapid Input on Smartwatches (ISWC 2016)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							Whoosh is an interaction technique using non-voice acoustic input including blows, sip-and-puff, and directional air swipes to enable low-cost, hands-free, and rapid input on smartwatches. Inspired by the design of musical instruments, we also develop a 3D-printed custom watch case to introduce directional and bezel blows without additional electronics. With the variety of vocabulary, Whoosh enables real-time discreet microinteractions on smartwatch.
                            <a href="https://dl.acm.org/doi/10.1145/2971763.2971765" >[Paper]</a>
							 
						</p>
                        <br />
				</div>
		</div>
        
        <div id="container">
			<div id="content2">
				<h1>Previous Projects</h1>
			</div>
		</div>

		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-csed.JPG" />
							 
						

						
							</div>
				<div id="content2">
						<h2>Applying Design Studio Pedagogy in STEM Learning with Novel Presentation and Sensing Technologies (IEEE VR 2015)</h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							I worked in a team to develop projected augmented reality to add design studio learning models to a classroom for STEM classes that encourage creativity, innovation, and help build strong peer learning environments. Students do classwork using an enhanced version of Pythy, a web IDE for Python and Jython, that captures students' work and displays it around the room. We leverage the Microsoft RoomAlive Toolkit to construct a room-scale augmented reality using pairs of projectors and depth cameras. The system "pins" students' work to the walls, where teachers and students can view, interact with, and discuss.
							<a href="http://ieeexplore.ieee.org/document/7504736/?arnumber=7504736&tag=1" >[Poster]</a>
						</p>
                        <br />
				</div>
		</div>
		<div id="container">
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-textile.png" />
							 
						

						
							</div>
				<div id="content2">
						<h2>Proprioceptive Interface: Enabling Eye-Free Interaction for Textile Buttons (2015) </h2>
						<!-- <img src="resources/thumb-auburn.png" alt="" border="0" title="demo" > -->
						<p>
							I have been investigating ways to facilitate eye-free interactions with textile buttons by using different types of techniques, including different vibrating patterns, various textures, and locking mechanisms. I am also experimenting different types of textile buttons including capacitive sensing, resistive sensing, and piezoelectric harvesting buttons.
						</p>
                        <br />
				</div>
		</div>


		<div id="container">
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
							<br /><br />
								<!-- <a href="resources/IUI paper.pdf" >paper</a>  --> 
								 <img src="resources/thumb-iui.png" />
							 
						

						
							</div>
				<div id="content2">
						<h2>BeyondTouch: Extending the Input Language with Built-in Sensors on Commodity Smartphones (IUI 2015)</h2>
						<p>
							I worked in a team to develop BeyondTouch, which extends and enriches smartphone inputs to a wide variety of additional tapping and sliding inputs on the case of and the surface adjacent to the smartphone, by using only existing sensing capabilities on a commodity smartphone. It can be applied to a variety of application scenarios.
                            <a href="https://www.youtube.com/watch?v=iT4-3Jv0IlU">[Video]</a> 
                            <a href="http://dl.acm.org/citation.cfm?id=2701374">[Paper]</a> 
						</p>
						<br />
				</div>
		</div>

<div id="container">
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-bci.png" />
							 
						

						
							</div>
				<div id="content2">
						
						<h2>Master Project -- Quadcopter Navigation Using Google Glass and Brain-Computer Interface (2015)</h2>
						<p>
							I developed assistive technologies for ALS patients to explore surroundings with wearable technology and a camera-mounted quadcopter. Google Glass is used to creating telepresence by displaying drone-retrieved first-person view, and presenting visual stimili for Steady-State Visually Evoked Potential (SSVEP). OpenBCI, a mobile Brain-Computer Interface, acquires user's electroencephalogram (EEG) for real-time analysis. Thus, user's attention to different icons presented on Glass is used to navigate the quadcopter wirelessly. Java, Android Studio, and Matlab are used in the project.
                            <a href="https://youtu.be/DeEF2F-8k8Y">[Video]</a>
						</p>
                        <br />
						
				</div>
		</div>
		
<!--
<div id="container">
				<div id="sidebar2">
						<br /><br />
						
								<a href="resources/Foodie-buddy-poster-clear3-page-001.jpg" ><img alt="Poster" title="Poster" src="resources/thumb-foodie.jpg" /></a>
							 
						

						
							</div>
				<div id="content2">
						
						<h2>UIST 2014 -- FoodieBuddy: Smart Cooking Assistant (2014)</h2>
						<p>
							I led a team to develop a smart cooking assistant which allows users to look up recipes while cooking, without worrying about oily hands on touchscreen. With Kinoma Create, web-based speech and recipe APIs, microphones, speakers, and proximity sensors, we implemented voice commands, audio navigation, and in-air gestures for touchless interactions and smart recipe recommendation. Javascript and HTML are used to develop UI and interact with RESTful APIs.
						</p>

						
				</div>
		</div>

		-->
		

<div id="container">
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
				<div id="sidebar2">
						
						<br /><br />
								<img src="resources/thumb-nasa.png" />
							 
						

						
							</div>
				<div id="content2">
						
						<h2>NASA Wearable Symposium 2014 -- Wearable Unobtrusive Noise Canceling Vest for ISS (2014)</h2>
						<p>
							I worked in a team to develop an unobtrusive wearable noise-canceling system for NASA astronauts onboard International Space Station (ISS) where very high level of noise is contantly generated from life support systems. Together we designed a vest with 3D printed adjustable collar integrated with circuit boards, speakers, microphones, and power supplies. I implemented  anti-noise generation in MAX/MSP, C, and Matlab for which could decrease various kinds of noise up to -10 dBA SPL.
                            <a href="resources/Nasa-poster.pdf" >[Poster]</a>
						</p>
						<br />
				</div>
		</div>

<div id="container">
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
						<br /><br />
								<img src="resources/thumb-horse.png" />
							 
						

						
							</div>
				<div id="content2">
						
						<h2>Dressage Horse Pattern Recognition (2014)</h2>
						<p>
							To help dressage riders analyze and review their performance, I developed approaches of wearable technologies, data analytics & visualization, and pattern recognition: Data are collected from sensors instrumented on rider and horse. Insights of the sport are revealed by signal processing and visualization techniques. Finally, a machine learning model is built to classify 10-class gaits with an overall accuracy of  97.4%. The development is done with Java, Android Studio, and Bluetooth Low Energy.
                            <a href="resources/Horse-Poster-page-001.jpg">[Poster]</a>
						</p>
                        <br />
				</div>
		</div>

<div id="container">
			<!-- <p>
							I am a Ph.D. student of Computer Science at Geogia Tech. Prior to this, I was a MSCS at Georgia Tech with a specilization in Human-Computer Interaction, and a BSCS student at Tsinghua University. My passion lies in inventing new technologies, in the field of Wearable and Mobile Computing, Augmented Reality, Machine Learning, and Music Technology. I also have several team leading experiences. 
						</p> -->
				<div id="sidebar2">
						
						<!-- <h1>Links</h1> -->
								<br /><br />
								<img src="resources/thumb-iccc.png" />
							 
						

						
							</div>
				<div id="content2">
						
						<h2>ICCC 2014 -- Building Artistic Computer Colleagues with an Enactive Model of Creativity (2014)</h2>
						<p>
							I worked on Computational Creativity which investigates ways to make computers generate creative products or use technology to support and enhance human creativity. We developed an aritificial intelligence program called Drawing Apprentice, which collaborates with human users as they draw on a digital canvas. The user and the program take turns to draw strokes, where the program learns the style of the human user and adds its own creativity .
                            <a href="http://computationalcreativity.net/iccc2014/wp-content/uploads/2014/06//2.3_Davis.pdf">[Paper]</a>    

						</p>
						<br />
				</div>
		</div>
		
</html>
<div id="footer">
			 </div>
</body>
</html>
